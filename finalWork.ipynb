{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/claudezyx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/claudezyx/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/claudezyx/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/claudezyx/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/claudezyx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: syllables in /home/claudezyx/anaconda3/lib/python3.7/site-packages (0.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import random \n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Below is only for local testing purpuse, not included in submissison\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Below model if not choosen is not necessary \n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "## Download Resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.corpus import words, stopwords\n",
    "#from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "############\n",
    "! pip install syllables\n",
    "import syllables\n",
    "import string\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendArticles(articleList, basePath):\n",
    "    contents = ''\n",
    "    for articleNumber in articleList:\n",
    "        f = open(basePath+str(articleNumber)+\".txt\", \"r\")\n",
    "        contents = f.read()+\";\"+contents\n",
    "        f.close()\n",
    "    return contents\n",
    "\n",
    "def stemTweetToWordList(text, stopWords):\n",
    "    ps = PorterStemmer()\n",
    "    tweet =text.lower() # lower case\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  \n",
    "    wordList = tokenizer.tokenize(tweet)\n",
    "     # remove stop words and store the stem version. \n",
    "    return [ps.stem(word) for word in wordList if word not in stopWords]\n",
    "\n",
    "def startNumber(text):\n",
    "    if text[:1].isdigit():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def containQuestion(text):\n",
    "    if '?' in text or '!' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def numberSyllable(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    number_words = len(text.split())\n",
    "    total = 0\n",
    "    for word in text.split():\n",
    "        total += syllables.estimate(word)\n",
    "    return total/number_words\n",
    "\n",
    "def assignLength(row, colName):\n",
    "    return len(row[colName])\n",
    "\n",
    "def POSTagging(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return nltk.pos_tag(tokens) \n",
    "\n",
    "def adjectives(lists):\n",
    "    count = 0\n",
    "    adjective_tag = ['JJ','JJR','JJS']\n",
    "    for word in lists:\n",
    "        if word[1] in adjective_tag:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def nouns(lists):\n",
    "    count = 0\n",
    "    noun_tag = ['NN','NNS','NNP']\n",
    "    for word in lists:\n",
    "        if word[1] in noun_tag:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def verbs(lists):\n",
    "    count = 0\n",
    "    verb_tag = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    for word in lists:\n",
    "        if word[1] in verb_tag:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def firstWord(lists):\n",
    "    tag = ['VB','VBD','VBG','VBN','VBP','VBZ','NN','NNS','NNP','JJ','JJR','JJS']\n",
    "    word = lists[0]\n",
    "    if word in tag:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def lastWord(lists):\n",
    "    tag = ['VB','VBD','VBG','VBN','VBP','VBZ','NN','NNS','NNP','JJ','JJR','JJS']\n",
    "    word = lists[-1]\n",
    "    if word in tag:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def FPP(lists):\n",
    "    count = 0\n",
    "    FPP_tag = ['I','ME','WE','US','MY','MINE','OUR','OURS']\n",
    "    for word in lists:\n",
    "        if word[0].upper() in FPP_tag:\n",
    "            count +=1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = os.path.dirname(os.path.abspath(\"train.json\"))\n",
    "# 0:false, 1:partly true, 2:true\n",
    "claim = pd.read_json(open(basePath + \"/train/train.json\", \"r\", encoding=\"utf8\"))\n",
    "txtPath = basePath+\"/train/train_articles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim['articleText'] = claim.apply(lambda row: appendArticles(row['related_articles'], txtPath) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "claim['articleText'] = claim.apply(lambda row: stemTweetToWordList(row['articleText'], stopWords) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "claim['claimSentiment'] = claim.apply(lambda row: sid.polarity_scores(row['claim'])['compound'] ,axis=1)\n",
    "claim['SentimentAdjust'] = claim['claimSentiment']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if claim starts with a number\n",
    "claim['start_number'] = claim.apply(lambda row: startNumber(row['claim']) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if claim contain any ? or !\n",
    "claim['containQX'] = claim.apply(lambda row: containQuestion(row['claim']) ,axis=1)\n",
    "#number of words in the claim\n",
    "claim['titleWords'] = claim.apply(lambda row: len(row['claim'].split()) ,axis=1)\n",
    "#number of related articles to the claim\n",
    "claim['numberArticle'] = claim.apply(lambda row: len(row['related_articles']) ,axis=1)\n",
    "#number of average syllables in each word in the claim\n",
    "claim['claimSyllable'] = claim.apply(lambda row: numberSyllable(row['claim']) ,axis=1)\n",
    "claim['articleLength'] = claim.apply(lambda row: assignLength(row, 'articleText'), axis=1)\n",
    "#average length of word in related articles to the claim\n",
    "#claim['articleLength'] = claim.apply(lambda row: assignLength(row, 'articleText')/row['numberArticle'], axis=1)\n",
    "\n",
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "#get number of punctuation in the claim\n",
    "claim['claimPunc'] = claim.apply(lambda row: count(row['claim'],set(string.punctuation)), axis=1)\n",
    "#get number of punctuation in each related articles\n",
    "claim['articlePunc'] = claim.apply(lambda row: count(row['articleText'],set(string.punctuation))/row['numberArticle'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of words and their POS for the claim\n",
    "claim['claimPOS'] = claim.apply(lambda row: POSTagging(row['claim']), axis=1)\n",
    "#check number of adjectives, nouns, and verbs in the claim\n",
    "claim['claimAdj'] = claim.apply(lambda row: adjectives(row['claimPOS']), axis=1)\n",
    "claim['claimNoun'] = claim.apply(lambda row: nouns(row['claimPOS']), axis=1)\n",
    "claim['claimVerb'] = claim.apply(lambda row: verbs(row['claimPOS']), axis=1)\n",
    "claim['claimPOSratio'] = claim.apply(lambda row: (row['claimAdj']+row['claimNoun']+row['claimVerb'])/len(row['claimPOS']), axis=1)\n",
    "claim['claimFirst'] = claim.apply(lambda row: firstWord(row['claimPOS']), axis=1)\n",
    "claim['claimLast'] = claim.apply(lambda row: lastWord(row['claimPOS']), axis=1)\n",
    "claim['claimFPP'] = claim.apply(lambda row: FPP(row['claimPOS']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    See full source and example: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim['articleText'] = claim.apply(lambda row: ' '.join(row['articleText']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \\\n",
    "                                   ngram_range=(1, 1), max_df=0.7, max_features=2500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.7, max_features=2500,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.fit(claim['articleText']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15555x2500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9161304 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train = tfidf_vectorizer.transform(claim['articleText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_df is 0.500000\n",
      "accuracy:   0.613\n",
      "accuracy:   0.612\n",
      "max_df is 0.700000\n",
      "accuracy:   0.612\n",
      "accuracy:   0.615\n",
      "max_df is 0.900000\n"
     ]
    }
   ],
   "source": [
    "max_df = 0.5\n",
    "while max_df < 1:\n",
    "#for numFeature in range(3000, 5000, 500):\n",
    "    #print('number of feature is %f' %numFeature)\n",
    "    print('max_df is %f' %max_df)\n",
    "    n_splits = 2\n",
    "    kfold = KFold(n_splits=n_splits)\n",
    "    kfold.get_n_splits(claim)\n",
    "    for train_idx, test_idx in kfold.split(claim):\n",
    "        X_train, X_test = claim.iloc[train_idx], claim.iloc[test_idx]    \n",
    "        y_train, y_test = X_train['label'], X_test['label']\n",
    "        X_train_text, X_test_text= X_train['articleText'], X_test['articleText']\n",
    "\n",
    "        ###\n",
    "#         #preprocess data\n",
    "#         normalizedList = ['numberArticle', 'articleLength', 'titleWords','claimPunc', 'articlePunc']\n",
    "#         for column in normalizedList:\n",
    "#             scaler = StandardScaler()\n",
    "#             X_train[column] = scaler.fit_transform(pd.DataFrame(X_train[column])) - \\\n",
    "#                                               min(scaler.fit_transform(pd.DataFrame(X_train[column])))\n",
    "#             X_test[column] = scaler.transform(pd.DataFrame(X_test[column])) - \\\n",
    "#                                               min(scaler.transform(pd.DataFrame(X_test[column])))\n",
    "#         ###\n",
    "\n",
    "        # Initialize the `tfidf_vectorizer` \n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english', \\\n",
    "                                           ngram_range=(1, 2), max_df=max_df, max_features=2500) \n",
    "        # Fit and transform the training data \n",
    "        tfidf_train = tfidf_vectorizer.fit_transform(X_train_text) \n",
    "        # Transform the test set \n",
    "        tfidf_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "        featureToBeAdded = ['SentimentAdjust','numberArticle','articleLength','start_number',\\\n",
    "                            'containQX','titleWords','claimSyllable','claimPunc','articlePunc',\\\n",
    "                            'claimAdj', 'claimNoun', 'claimVerb', 'claimPOSratio', 'claimFirst',\\\n",
    "                            'claimLast', 'claimFPP',\n",
    "                            #'claimSentiment', 'claimPOS'\n",
    "                           ]\n",
    "\n",
    "        combResults = tfidf_train\n",
    "        for featureName in featureToBeAdded:\n",
    "            colToBeAdded = coo_matrix(X_train[featureName]).transpose()\n",
    "            combResults = hstack([combResults, colToBeAdded])\n",
    "\n",
    "        combResultsTest = tfidf_test\n",
    "        for featureName in featureToBeAdded:\n",
    "            colToBeAdded = coo_matrix(X_test[featureName]).transpose()\n",
    "            combResultsTest = hstack([combResultsTest, colToBeAdded])\n",
    "\n",
    "        #multi_class : str, {‘ovr’, ‘multinomial’, ‘auto’}, optional (default=’ovr’)\n",
    "        #solver : str, {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, optional (default=’liblinear’).\n",
    "        clf = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial', max_iter=10000)\n",
    "        #clf = MultinomialNB(alpha=alpha) \n",
    "        #clf = ComplementNB()\n",
    "        #clf =  SVC()\n",
    "        #clf = PassiveAggressiveClassifier(tol=50)\n",
    "        clf.fit(combResults, y_train)\n",
    "        pred = clf.predict(combResultsTest)\n",
    "        score = accuracy_score(y_test, pred)\n",
    "        print(\"accuracy:   %0.3f\" % score)\n",
    "        #cm = confusion_matrix(y_test, pred, labels=[0, 1, 2])\n",
    "        #plot_confusion_matrix(cm, classes=[0, 1, 2])\n",
    "        max_df += 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_splits = 2\n",
    "# kfold = KFold(n_splits=n_splits)\n",
    "# kfold.get_n_splits(claim)\n",
    "# for train_idx, test_idx in kfold.split(claim):\n",
    "#     X_train, X_test = claim.iloc[train_idx], claim.iloc[test_idx]    \n",
    "#     y_train, y_test = X_train['label'], X_test['label']\n",
    "#     X_train_text, X_test_text= X_train['articleText'], X_test['articleText']\n",
    "    \n",
    "#     # Initialize the `tfidf_vectorizer` \n",
    "#     tfidf_vectorizer = TfidfVectorizer(stop_words='english', \\\n",
    "#                                        ngram_range=(2, 2), max_df=0.7, max_features=3000) \n",
    "#     # Fit and transform the training data \n",
    "#     tfidf_train = tfidf_vectorizer.fit_transform(X_train_text) \n",
    "#     # Transform the test set \n",
    "#     tfidf_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "#     featureToBeAdded = ['SentimentAdjust',\\\n",
    "#                         #'claimSentiment',\\\n",
    "#                         'numberArticle','articleLength',\\\n",
    "#                         'start_number',\\\n",
    "#                         'containQX', \\\n",
    "#                         'titleWords',\\\n",
    "#                         'claimSyllable',\\\n",
    "#                         'claimPunc', 'articlePunc',\\\n",
    "#                        ]\n",
    "#     combResults = tfidf_train\n",
    "#     for featureName in featureToBeAdded:\n",
    "#         colToBeAdded = coo_matrix(X_train[featureName]).transpose()\n",
    "#         combResults = hstack([combResults, colToBeAdded])\n",
    "\n",
    "#     combResultsTest = tfidf_test\n",
    "#     for featureName in featureToBeAdded:\n",
    "#         colToBeAdded = coo_matrix(X_test[featureName]).transpose()\n",
    "#         combResultsTest = hstack([combResultsTest, colToBeAdded])\n",
    "\n",
    "#     #multi_class : str, {‘ovr’, ‘multinomial’, ‘auto’}, optional (default=’ovr’)\n",
    "#     #solver : str, {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, optional (default=’liblinear’).\n",
    "#     clf = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial')\n",
    "#     #clf = MultinomialNB() \n",
    "#     #clf = ComplementNB()\n",
    "#     #clf =  SVC()\n",
    "#     #clf = PassiveAggressiveClassifier(tol=50)\n",
    "#     clf.fit(combResults, y_train)\n",
    "#     pred = clf.predict(combResultsTest)\n",
    "#     score = accuracy_score(y_test, pred)\n",
    "#     print(\"accuracy:   %0.3f\" % score)\n",
    "#     cm = confusion_matrix(y_test, pred, labels=[0, 1, 2])\n",
    "#     plot_confusion_matrix(cm, classes=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of feature is 1500.000000\n",
    "#accuracy:   0.612\n",
    "#accuracy:   0.611\n",
    "#number of feature is 2000.000000\n",
    "#accuracy:   0.612\n",
    "#accuracy:   0.617\n",
    "#number of feature is 2500.000000\n",
    "# number of feature is 3000.000000\n",
    "# accuracy:   0.614\n",
    "# accuracy:   0.620\n",
    "# number of feature is 3500.000000\n",
    "# accuracy:   0.617\n",
    "# accuracy:   0.619\n",
    "# number of feature is 4000.000000\n",
    "# accuracy:   0.616\n",
    "# accuracy:   0.619\n",
    "# number of feature is 4500.000000\n",
    "# accuracy:   0.614\n",
    "# accuracy:   0.618"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
