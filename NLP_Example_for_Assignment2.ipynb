{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing Example\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pingwu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/54/1d7294672110d5c0565cabc4b99ed952ced9a2dc2ca1d59ad1b34303a6de/gensim-3.8.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.7MB 464kB/s ta 0:00:01   35% |███████████▎                    | 8.7MB 1.9MB/s eta 0:00:09\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Users/pingwu/anaconda3/lib/python3.7/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/pingwu/anaconda3/lib/python3.7/site-packages (from gensim) (1.15.4)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/09/735f2786dfac9bbf39d244ce75c0313d27d4962e71e0774750dc809f2395/smart_open-1.9.0.tar.gz (70kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.4MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /Users/pingwu/Library/Python/3.7/lib/python/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: boto>=2.32 in /Users/pingwu/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /Users/pingwu/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/a9/1ceaeda8aa5d3effc9098ae301820e27bf54c4000ec6f8ec79f9b265c50e/boto3-1.10.19-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 2.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/pingwu/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/pingwu/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/pingwu/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pingwu/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2018.11.29)\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting botocore<1.14.0,>=1.13.19 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/e5/0f29669244ffacc15c4cec9e10d75c26e7d300e1786e79514e62373e648c/botocore-1.13.19-py2.py3-none-any.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 1.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/pingwu/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.19->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /Users/pingwu/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.19->boto3->smart-open>=1.8.1->gensim) (2.7.5)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/pingwu/Library/Caches/pip/wheels/ab/10/93/5cff86f5b721d77edaecc29959b1c60d894be1f66d91407d28\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.10.19 botocore-1.13.19 gensim-3.8.1 jmespath-0.9.4 s3transfer-0.2.1 smart-open-1.9.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "try:\n",
    "    from gensim.models import word2vec\n",
    "except:\n",
    "    !pip install gensim\n",
    "    from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "We have 5 examples of documents (tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "'All bears are lovely',\n",
    "'Our tea was bad',\n",
    "'That bear drinks with bear',\n",
    "'The bear drinks tea',\n",
    "'We love bears'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #1: \"All bears are lovely\"\n",
      "Example #2: \"Our tea was bad\"\n",
      "Example #3: \"That bear drinks with bear\"\n",
      "Example #4: \"The bear drinks tea\"\n",
      "Example #5: \"We love bears\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    print('Example #{0:d}: \"{1:s}\"'.format(i+1,corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data\n",
    "\n",
    "Convert to lower case, remove stop words, stem words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lc = []\n",
    "s1 = ' '\n",
    "corpus_clean = []\n",
    "for line in corpus:\n",
    "    lower_case = line.lower() # lowercase \n",
    "    list_lc.append(lower_case)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')         \n",
    "    b = tokenizer.tokenize(lower_case)\n",
    "    words_rmStop = [word for word in b if word not in stopwords.words('english')] # remove stop words\n",
    "    ps = PorterStemmer()\n",
    "    words_stem = [ps.stem(word) for word in words_rmStop] # stem \n",
    "    corpus_clean.append(s1.join(words_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bear love', 'tea bad', 'bear drink bear', 'bear drink tea', 'love bear']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned example #1: \"bear love\"\n",
      "Cleaned example #2: \"tea bad\"\n",
      "Cleaned example #3: \"bear drink bear\"\n",
      "Cleaned example #4: \"bear drink tea\"\n",
      "Cleaned example #5: \"love bear\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    print('Cleaned example #{0:d}: \"{1:s}\"'.format(i+1,corpus_clean[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency (WF)\n",
    "\n",
    "The \"word frequency\" (WF) method records the number of times that term occurs in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bear love', 'tea bad', 'bear drink bear', 'bear drink tea', 'love bear']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0 1 0 1 0]\n",
      " [1 0 0 0 1]\n",
      " [0 2 1 0 0]\n",
      " [0 1 1 0 1]\n",
      " [0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "array_WF = vectorizer.fit_transform(corpus_clean).toarray()\n",
    "#print(vectorizer.vocabulary_)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(array_WF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results for the \"bag of words\" (WF) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bear': 1, 'love': 3, 'tea': 4, 'bad': 0, 'drink': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tbad    bear   drink  love   tea    \t\n",
      "Example #1      0      1      0      1      0         \"bear love\"\t\n",
      "Example #2      1      0      0      0      1         \"tea bad\"\t\n",
      "Example #3      0      2      1      0      0         \"bear drink bear\"\t\n",
      "Example #4      0      1      1      0      1         \"bear drink tea\"\t\n",
      "Example #5      0      1      0      1      0         \"love bear\"\t\n"
     ]
    }
   ],
   "source": [
    "#import operator\n",
    "#sorted_voc = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "sorted_voc = vectorizer.get_feature_names()\n",
    "print('\\t\\t', end = '')\n",
    "for j in range(len(vectorizer.vocabulary_)):\n",
    "    print('{0:7s}'.format(sorted_voc[j]), end = '')\n",
    "    #print('{0:7s}'.format(sorted_voc[j][0]), end = '')\n",
    "print('\\t')\n",
    "for j in range(len(vectorizer.vocabulary_)):\n",
    "    print('Example #{0:d}'.format(j+1), end = '')\n",
    "    for i in range(array_WF.shape[0]):\n",
    "        print('{0:7d}'.format(array_WF[j][i]), end = '')\n",
    "    print('         \"{0:s}\"\\t'.format(corpus_clean[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency method is used in order to reduce influence of a document length.\n",
    "\n",
    "The way to calculate it: $\\frac{\\rm Word ~ Frequency}{\\rm total ~ number ~ of ~ words ~ in ~ the ~ document}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0.         0.5        0.         0.5        0.        ]\n",
      " [0.5        0.         0.         0.         0.5       ]\n",
      " [0.         0.66666667 0.33333333 0.         0.        ]\n",
      " [0.         0.33333333 0.33333333 0.         0.33333333]\n",
      " [0.         0.5        0.         0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "array_TF = array_WF/array_WF.sum(axis=1,keepdims=True)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(array_TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get the same results using `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0.         0.5        0.         0.5        0.        ]\n",
      " [0.5        0.         0.         0.         0.5       ]\n",
      " [0.         0.66666667 0.33333333 0.         0.        ]\n",
      " [0.         0.33333333 0.33333333 0.         0.33333333]\n",
      " [0.         0.5        0.         0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(use_idf=False, norm=\"l1\")\n",
    "array_TF1 = vectorizer2.fit_transform(corpus_clean).toarray()\n",
    "print(vectorizer2.get_feature_names())\n",
    "print( array_TF1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results for the \"term frequency\" (TF) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t     bad    bear   drink  love   tea    \t\n",
      "Example #1   0.00   0.50   0.00   0.50   0.00         \"bear love\"\t\n",
      "Example #2   0.50   0.00   0.00   0.00   0.50         \"tea bad\"\t\n",
      "Example #3   0.00   0.67   0.33   0.00   0.00         \"bear drink bear\"\t\n",
      "Example #4   0.00   0.33   0.33   0.00   0.33         \"bear drink tea\"\t\n",
      "Example #5   0.00   0.50   0.00   0.50   0.00         \"love bear\"\t\n"
     ]
    }
   ],
   "source": [
    "sorted_voc = vectorizer2.get_feature_names()\n",
    "print('\\t     ', end = '')\n",
    "for j in range(len(vectorizer2.vocabulary_)):\n",
    "    print('{0:7s}'.format(sorted_voc[j]), end = '')\n",
    "print('\\t')\n",
    "for j in range(len(vectorizer2.vocabulary_)):\n",
    "    print('Example #{0:d}'.format(j+1), end = '')\n",
    "    for i in range(array_TF1.shape[0]):\n",
    "        print('{0:7.2f}'.format(array_TF1[j][i]), end = '')\n",
    "    print('         \"{0:s}\"\\t'.format(corpus_clean[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency–Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "The formula that is used to compute the $\\mbox{tf-idf }$ of term $t$ is\n",
    "\n",
    "$\\mbox{tf-idf}(d, t) = \\mbox{tf}(t) \\cdot \\mbox{idf}(d, t)$\n",
    "\n",
    "There are a number of ways to calculate $\\mbox{tf}$ and $\\mbox{idf}$. According to `TfidfVectorizer` documentation\n",
    "\n",
    "$\\mbox{tf}(t)$ here is word frequency,\n",
    "\n",
    "if `smooth_idf=False`, \n",
    "$\\mbox{idf}$ is computed as $\\mbox{idf}(d, t) = \\log \\left[ \\frac{n}{{\\rm df}(d, t)} \\right] + 1$,\n",
    "\n",
    "if `smooth_idf=True`, \n",
    "$\\mbox{idf}$ is computed as $\\mbox{idf}(d, t) = \\log \\left[ \\frac{ 1+n }{ 1+{\\rm df}(d, t) } \\right] + 1$,\n",
    "\n",
    "where $n$ is the total number of documents and $\\mbox{df}(d, t)$ is the document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0.         1.18232156 0.         1.69314718 0.        ]\n",
      " [2.09861229 0.         0.         0.         1.69314718]\n",
      " [0.         2.36464311 1.69314718 0.         0.        ]\n",
      " [0.         1.18232156 1.69314718 0.         1.69314718]\n",
      " [0.         1.18232156 0.         1.69314718 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None)\n",
    "array_TFIDF = vectorizer3.fit_transform(corpus_clean).toarray()\n",
    "print( vectorizer3.get_feature_names() )\n",
    "print( array_TFIDF )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results for the \"term frequency - inverse document frequency\" (TF-IDF) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t     bad    bear   drink  love   tea    \t\n",
      "Example #1   0.00   1.18   0.00   1.69   0.00         \"bear love\"\t\n",
      "Example #2   2.10   0.00   0.00   0.00   1.69         \"tea bad\"\t\n",
      "Example #3   0.00   2.36   1.69   0.00   0.00         \"bear drink bear\"\t\n",
      "Example #4   0.00   1.18   1.69   0.00   1.69         \"bear drink tea\"\t\n",
      "Example #5   0.00   1.18   0.00   1.69   0.00         \"love bear\"\t\n"
     ]
    }
   ],
   "source": [
    "sorted_voc = vectorizer3.get_feature_names()\n",
    "print('\\t     ', end = '')\n",
    "for j in range(len(vectorizer3.vocabulary_)):\n",
    "    print('{0:7s}'.format(sorted_voc[j]), end = '')\n",
    "print('\\t')\n",
    "for j in range(len(vectorizer3.vocabulary_)):\n",
    "    print('Example #{0:d}'.format(j+1), end = '')\n",
    "    for i in range(array_TFIDF.shape[0]):\n",
    "        print('{0:7.2f}'.format(array_TFIDF[j][i]), end = '')\n",
    "    print('         \"{0:s}\"\\t'.format(corpus_clean[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\n",
    "\n",
    "Here we use `Word2Vec` as example.\n",
    "There are a lot of ways to use the word embedding as features, here we use joining (averaging) vectors from the words from sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [sentence.split() for sentence in corpus_clean]\n",
    "model = word2vec.Word2Vec(tokenized_sentences, size=100, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-05 16:31:55,582 [26252] WARNING  py.warnings:110: [JupyterRequire] C:\\Users\\OLEKSANDRRomanko\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tea', 0.04989343136548996),\n",
       " ('bear', -0.03948143869638443),\n",
       " ('bad', -0.04141325503587723),\n",
       " ('drink', -0.06769246608018875)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pingwu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-4.5096301e-03,  1.0776015e-03,  2.3104686e-03,  3.7820239e-03,\n",
       "         1.5000100e-03, -6.6087674e-04,  1.3785056e-03,  1.1966325e-03,\n",
       "        -1.6633301e-03,  3.0567958e-03,  8.8104384e-04, -2.5930733e-03,\n",
       "        -2.1055257e-03,  1.5693693e-03,  4.0840977e-03,  3.7100532e-03,\n",
       "        -4.1637695e-03, -2.4407684e-04, -3.6076580e-03, -4.0282798e-03,\n",
       "         1.2396622e-03,  2.3122001e-03, -6.1124103e-04, -1.1780466e-03,\n",
       "         2.1181912e-03, -4.7766850e-03, -3.7995996e-03,  2.7426230e-04,\n",
       "        -3.3462713e-03, -1.8063164e-03,  3.1984197e-03,  8.4785663e-04,\n",
       "        -8.8840068e-05, -2.0279523e-03, -2.7868808e-03, -2.9570975e-03,\n",
       "         1.6235323e-04,  1.4239192e-03, -3.9102710e-03, -8.0130203e-06,\n",
       "        -3.0255269e-03, -1.2272932e-03, -2.5831209e-03,  1.4058248e-03,\n",
       "         2.7082025e-03,  4.6033827e-03,  1.8440962e-03, -2.0039440e-04,\n",
       "         6.0292627e-05, -4.6431632e-03,  3.1464512e-03, -3.3449712e-03,\n",
       "        -1.6436554e-03,  2.3879504e-03, -2.6112557e-03, -3.6032738e-03,\n",
       "        -3.9650700e-03,  1.3410077e-04, -4.5785052e-03, -6.4667087e-04,\n",
       "        -2.3098234e-03, -4.4294130e-03,  2.7211853e-03, -1.1701242e-03,\n",
       "        -1.3538316e-03, -1.2656582e-03,  2.2311644e-03, -6.8964669e-04,\n",
       "         5.1179103e-04,  1.9222848e-03,  4.9208093e-04,  2.6934015e-04,\n",
       "        -4.2274022e-03,  1.5270607e-04, -4.3546883e-03, -3.0197759e-04,\n",
       "         7.0674461e-05,  2.4120251e-03,  6.3913641e-04, -2.3739347e-03,\n",
       "        -4.0680948e-03,  1.7921561e-03, -3.4169003e-03, -4.3475413e-04,\n",
       "         2.4511782e-03,  2.6324759e-03,  7.2036724e-04,  1.2788066e-03,\n",
       "         1.9421072e-04,  6.8741018e-04,  6.7388319e-04, -2.7026497e-03,\n",
       "        -3.4729650e-03,  3.6402382e-03, -1.2621388e-03, -5.4439024e-05,\n",
       "         4.8227503e-04,  8.4283488e-04, -1.5635407e-03,  2.4601405e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['love'].reshape((1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    text = text.split(' ')\n",
    "    for word in text:\n",
    "        vec += model[word].reshape((1, size))\n",
    "        count += 1.\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bear love', 'tea bad', 'bear drink bear', 'bear drink tea', 'love bear']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pingwu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "array_wordEmbedding = np.concatenate([buildWordVector(z, 100) for z in corpus_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.95494192e-04  2.14229617e-03  1.00282682e-03  2.30378966e-03\n",
      "  -8.36473424e-04 -2.57339864e-03  1.59368583e-03  1.05653459e-03\n",
      "  -3.01944680e-03  2.44038715e-03  2.82365625e-03 -2.24264094e-03\n",
      "   1.36526139e-03  1.20678419e-03  3.56960949e-03  3.29339586e-03\n",
      "   2.51698773e-04 -1.23027073e-03 -4.21406853e-03 -4.08171886e-03\n",
      "   7.97631496e-04 -1.16096553e-03 -1.95844870e-03 -2.65685952e-03\n",
      "  -6.01756270e-04 -1.44713558e-05 -6.34645461e-04  1.57335617e-03\n",
      "  -4.47480939e-04 -3.12123448e-03  2.81179172e-03 -2.81706394e-04\n",
      "  -1.59697889e-03 -1.42891498e-03 -1.91793521e-03 -4.35673050e-04\n",
      "  -6.55694595e-04  2.81806762e-03 -3.79118766e-03  4.89779380e-04\n",
      "  -2.86811881e-03  2.06357916e-04 -2.32828106e-03  2.30017578e-03\n",
      "   3.03848495e-03  1.08424074e-03 -1.69115840e-04  8.65153634e-04\n",
      "   2.33931500e-03 -3.80504271e-03  3.38211865e-03 -2.68545514e-03\n",
      "   2.67913449e-04  3.49915738e-03  5.33820363e-04  5.95182646e-05\n",
      "  -4.95149288e-05 -5.64630151e-04 -4.62433370e-03 -2.47629781e-03\n",
      "  -2.15658860e-03 -2.98732822e-03  3.90278175e-04  1.17763132e-03\n",
      "  -4.09498462e-04 -7.46189449e-04  5.59692213e-04 -1.30401022e-03\n",
      "  -3.62814317e-04 -1.07663189e-03 -6.73987786e-04 -1.98927020e-03\n",
      "  -2.74272118e-03 -5.62733847e-04  1.96894864e-04 -2.10611241e-03\n",
      "  -2.34971679e-03  1.67426394e-03  2.20637140e-03  1.47656654e-04\n",
      "  -2.98550451e-03  1.08655894e-03 -3.24115949e-03  1.34229718e-04\n",
      "   1.75348180e-03  1.02554963e-03 -1.04102402e-03  9.12388583e-04\n",
      "   1.23232984e-03 -6.09681621e-04  2.13922610e-03  4.03624261e-04\n",
      "  -3.95518856e-03  3.44798586e-03 -1.13441970e-03 -1.40979209e-03\n",
      "   6.11331197e-04 -8.55803577e-04 -1.12996635e-03 -1.13890518e-03]\n",
      " [-5.99589664e-04  1.46614038e-04  5.47318603e-04  2.57106176e-03\n",
      "  -4.47075581e-04 -7.64959841e-04 -9.71961650e-04 -1.88524814e-03\n",
      "  -9.89290100e-04 -3.59751086e-03  1.69396633e-04 -4.45914967e-03\n",
      "  -1.80848364e-03 -4.63123224e-03 -2.85150728e-03 -1.04356033e-03\n",
      "   5.11175371e-04 -1.75369886e-03  2.95847631e-03  1.09688996e-03\n",
      "   2.22745293e-03 -8.94212077e-04 -5.97486855e-04 -4.20067378e-03\n",
      "   2.12151837e-03 -1.66218114e-03 -3.50657583e-03  4.10337118e-04\n",
      "  -1.48733312e-03  3.08222970e-03 -2.18529836e-03  1.16960524e-03\n",
      "   9.67447384e-04 -2.49543227e-03  2.40027826e-03 -2.20018551e-03\n",
      "  -1.93916267e-03 -6.93271868e-05  2.39241449e-03  3.66449798e-03\n",
      "  -1.96053647e-03  8.33873637e-05 -3.84827505e-03  1.02387017e-03\n",
      "   6.60013058e-04  6.41534105e-04 -6.72239228e-04  1.38750178e-03\n",
      "   2.41750886e-03 -2.08350463e-03 -1.30153453e-03 -2.80644756e-03\n",
      "   3.30844987e-03 -1.05216264e-03 -2.03297579e-03 -9.16282937e-04\n",
      "   1.15459342e-03 -5.83158457e-04  9.39058518e-04 -2.97303312e-04\n",
      "   6.28341222e-04  7.45213867e-04  1.79236836e-03 -4.83432552e-04\n",
      "   9.09638125e-04 -1.87578454e-03 -3.41978250e-03  1.85101287e-03\n",
      "   5.84173249e-04 -1.62044744e-03 -4.04813280e-03 -6.68525870e-04\n",
      "   1.84773596e-03 -7.26172701e-04 -2.26304057e-03 -1.55173024e-03\n",
      "  -3.58523231e-03 -5.73642552e-04  2.10411841e-03 -4.90184262e-04\n",
      "  -3.38642922e-03 -1.74282145e-03  9.35165444e-05 -7.51510815e-04\n",
      "  -2.32514873e-03 -1.56486194e-03  9.31724499e-04 -8.75927304e-04\n",
      "  -3.22954624e-03 -2.26841176e-03  3.32167081e-03  2.32483959e-04\n",
      "   5.14158397e-04 -1.22941690e-03  1.58916973e-03 -2.71994137e-03\n",
      "  -1.69338979e-03  5.83299377e-04  1.88870460e-03  2.79114780e-03]\n",
      " [ 2.30857873e-03  1.75551554e-03 -2.81509803e-04 -2.71668192e-04\n",
      "  -1.51213907e-03 -3.82019059e-03  8.25095922e-05  1.75140930e-03\n",
      "  -3.86639459e-03  2.58997502e-03  2.92228675e-03 -9.97559517e-04\n",
      "   4.38584597e-03  5.19048796e-04  1.36920957e-03  2.78959020e-03\n",
      "   3.05830906e-03 -2.03715637e-04 -1.85980632e-03 -2.03035570e-03\n",
      "  -2.41802016e-04 -2.99470878e-03 -5.81648977e-04 -3.08811960e-03\n",
      "  -1.50397094e-03  3.46042105e-03  9.29792256e-04  3.35396593e-03\n",
      "   2.27785720e-03 -3.87747384e-03  1.13773951e-03 -1.80726498e-03\n",
      "  -2.37603934e-03  6.04584386e-04 -2.29725226e-03  2.14483915e-03\n",
      "  -7.55430781e-04  3.32040340e-03 -2.13985745e-03  3.98026663e-04\n",
      "  -3.30376610e-03  1.32712202e-03 -1.82287220e-03  3.12237053e-03\n",
      "   1.07920651e-03 -2.11915987e-03 -2.11277728e-03  1.50804144e-03\n",
      "   2.61945669e-03 -1.37869517e-03  2.29178628e-03 -8.77554994e-05\n",
      "   7.36123572e-05  4.57645115e-03  1.20154854e-03  2.71982143e-03\n",
      "   4.08491430e-03 -1.88324057e-04 -2.97348289e-03 -3.08471070e-03\n",
      "  -1.52330537e-03 -1.90737541e-03 -2.44599084e-03  1.50037293e-03\n",
      "   5.83877166e-05 -8.05797686e-04 -1.96568598e-03 -6.10822036e-04\n",
      "   7.99498055e-04 -3.08331964e-03 -2.63314003e-03 -2.15634890e-03\n",
      "  -9.47266051e-04 -1.71546599e-03  1.69964973e-03 -1.15255686e-03\n",
      "  -2.79703845e-03  1.26255513e-03  1.04101390e-03  1.85893693e-04\n",
      "  -1.12530089e-03 -1.38934900e-03 -2.33578143e-03  2.05568647e-03\n",
      "   3.94920508e-04  1.25822336e-03 -3.35590510e-03 -3.82127201e-04\n",
      "   2.69029445e-04  6.92612957e-05  3.20264356e-03  3.87998469e-03\n",
      "  -1.35070303e-03  8.04711754e-04  9.72501002e-04 -2.62362906e-03\n",
      "   1.40918566e-03 -2.56517809e-03 -1.88967927e-03 -3.30994747e-03]\n",
      " [ 2.47269097e-03  1.90238084e-03  7.74470784e-04  1.05100362e-03\n",
      "  -8.64342942e-04 -2.31457412e-03 -2.11487823e-03  1.16410277e-05\n",
      "  -3.38389430e-03  1.07295245e-03  6.43246497e-04 -1.71023596e-03\n",
      "   1.59448556e-03 -1.27602046e-03 -5.78046776e-04  1.48200170e-03\n",
      "   2.50499441e-03 -9.80679023e-04  2.21795092e-04  3.69658771e-04\n",
      "   1.01715705e-04 -1.54500347e-03  3.89386706e-04 -3.27629456e-03\n",
      "   1.93125258e-05  5.44487014e-04 -7.49108537e-04  1.10618056e-03\n",
      "   7.18554753e-04 -9.75433892e-04 -6.17392284e-04 -1.29400908e-04\n",
      "  -8.56375729e-04 -9.34409327e-05 -4.07093825e-04  1.22153355e-04\n",
      "  -1.27541794e-03  2.99304576e-03 -4.58439676e-04  1.40131917e-03\n",
      "  -3.25687532e-03 -1.82571200e-04 -2.70062759e-03  2.94849540e-03\n",
      "  -6.88361392e-04  1.97424709e-04 -2.30275141e-03  1.89920697e-03\n",
      "   1.75342992e-03 -1.03653948e-03  2.53300677e-04 -3.55533712e-04\n",
      "   2.58200957e-04  2.79857150e-03 -1.36658899e-03  2.04327421e-03\n",
      "   3.17904342e-03  1.24695559e-03 -1.73906602e-03 -1.21094793e-03\n",
      "  -1.00039623e-03 -1.47896820e-03 -1.36665689e-04 -1.10955482e-03\n",
      "  -1.16410040e-03 -5.18223004e-04 -2.40445374e-03  1.28433555e-03\n",
      "   2.87479108e-03 -1.32591545e-03 -3.58884817e-03 -1.81211446e-03\n",
      "   1.50782085e-04 -3.62576024e-04 -2.11709839e-04  2.90425941e-04\n",
      "  -2.19559507e-03  1.79735208e-03  8.78515498e-04 -4.89706736e-04\n",
      "  -1.09721896e-03 -1.30316537e-03 -8.40787388e-04  2.14224126e-03\n",
      "  -1.85292020e-04  1.01446504e-04 -2.87347772e-03 -8.23468183e-04\n",
      "  -1.39419789e-03  6.88633559e-04  3.25058579e-03  4.29232558e-03\n",
      "  -1.04393585e-03 -7.56860051e-05  1.68591365e-03 -2.18510965e-03\n",
      "   2.74722503e-04 -1.02428626e-03 -7.72691332e-04 -9.87518608e-04]\n",
      " [-9.95494192e-04  2.14229617e-03  1.00282682e-03  2.30378966e-03\n",
      "  -8.36473424e-04 -2.57339864e-03  1.59368583e-03  1.05653459e-03\n",
      "  -3.01944680e-03  2.44038715e-03  2.82365625e-03 -2.24264094e-03\n",
      "   1.36526139e-03  1.20678419e-03  3.56960949e-03  3.29339586e-03\n",
      "   2.51698773e-04 -1.23027073e-03 -4.21406853e-03 -4.08171886e-03\n",
      "   7.97631496e-04 -1.16096553e-03 -1.95844870e-03 -2.65685952e-03\n",
      "  -6.01756270e-04 -1.44713558e-05 -6.34645461e-04  1.57335617e-03\n",
      "  -4.47480939e-04 -3.12123448e-03  2.81179172e-03 -2.81706394e-04\n",
      "  -1.59697889e-03 -1.42891498e-03 -1.91793521e-03 -4.35673050e-04\n",
      "  -6.55694595e-04  2.81806762e-03 -3.79118766e-03  4.89779380e-04\n",
      "  -2.86811881e-03  2.06357916e-04 -2.32828106e-03  2.30017578e-03\n",
      "   3.03848495e-03  1.08424074e-03 -1.69115840e-04  8.65153634e-04\n",
      "   2.33931500e-03 -3.80504271e-03  3.38211865e-03 -2.68545514e-03\n",
      "   2.67913449e-04  3.49915738e-03  5.33820363e-04  5.95182646e-05\n",
      "  -4.95149288e-05 -5.64630151e-04 -4.62433370e-03 -2.47629781e-03\n",
      "  -2.15658860e-03 -2.98732822e-03  3.90278175e-04  1.17763132e-03\n",
      "  -4.09498462e-04 -7.46189449e-04  5.59692213e-04 -1.30401022e-03\n",
      "  -3.62814317e-04 -1.07663189e-03 -6.73987786e-04 -1.98927020e-03\n",
      "  -2.74272118e-03 -5.62733847e-04  1.96894864e-04 -2.10611241e-03\n",
      "  -2.34971679e-03  1.67426394e-03  2.20637140e-03  1.47656654e-04\n",
      "  -2.98550451e-03  1.08655894e-03 -3.24115949e-03  1.34229718e-04\n",
      "   1.75348180e-03  1.02554963e-03 -1.04102402e-03  9.12388583e-04\n",
      "   1.23232984e-03 -6.09681621e-04  2.13922610e-03  4.03624261e-04\n",
      "  -3.95518856e-03  3.44798586e-03 -1.13441970e-03 -1.40979209e-03\n",
      "   6.11331197e-04 -8.55803577e-04 -1.12996635e-03 -1.13890518e-03]]\n"
     ]
    }
   ],
   "source": [
    "print( array_wordEmbedding )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
